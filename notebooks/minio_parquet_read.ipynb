{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106b3c2b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIOParquetRead\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.parquet(\"s3a://purchased-items/\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd47227",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col, sum as spark_sum, desc, countDistinct, count\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, ArrayType, TimestampType\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Spark session oluştur (daha önce oluşturmadıysan)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIOParquetRead\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Parquet dosyalarını oku\n",
    "df = spark.read.parquet(\"s3a://purchased-items/\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292f4556",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, sum as spark_sum\n",
    "\n",
    "# Products array'ini explode et\n",
    "products_exploded = df.select(\"UserId\", \"OrderId\", explode(\"Products\").alias(\"Product\"))\n",
    "\n",
    "# Ürün bazında toplam satılan adetleri hesapla\n",
    "product_sales = products_exploded.groupBy(\"Product.ProductId\") \\\n",
    "    .agg(spark_sum(\"Product.ItemCount\").alias(\"TotalSold\")) \\\n",
    "    .orderBy(spark_sum(\"Product.ItemCount\").desc())\n",
    "\n",
    "product_sales.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c365bf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "payment_type_counts = df.groupBy(\"PaymentType\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "payment_type_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e418c0a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col, current_timestamp\n",
    "\n",
    "recent_orders_1h = df.filter(\n",
    "    (col(\"TimeStamp\") >= (current_timestamp() - expr(\"INTERVAL 1 HOURS\"))) &\n",
    "    (col(\"TimeStamp\") <= current_timestamp())\n",
    ")\n",
    "\n",
    "recent_orders_1h.show()\n",
    "\n",
    "top_customers_1h = recent_orders_1h.groupBy(\"UserId\") \\\n",
    "    .agg(spark_sum(\"TotalPrice\").alias(\"TotalSpent\")) \\\n",
    "    .orderBy(col(\"TotalSpent\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "top_customers_1h.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b3605a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.selectExpr(\"min(TimeStamp)\", \"max(TimeStamp)\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bd8c6b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"s3a://purchased-items/\")\n",
    "df.printSchema()  # Kolonları kontrol et\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379e3ce1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, sum as spark_sum, col, countDistinct\n",
    "\n",
    "# products_exploded zaten df üzerinden oluşturuldu varsayımıyla devam ediyorum.\n",
    "# Eğer df değişkenin MinIO'dan okunan orijinal DataFrame değilse, önceki adımları kontrol et.\n",
    "# products_exploded = df.select(\"UserId\", explode(\"Products\").alias(\"Product\"))\n",
    "\n",
    "# Müşteri ve ürün bazında toplam satın alma sayısı\n",
    "customer_product_counts = products_exploded.groupBy(\"UserId\", \"Product.ProductId\") \\\n",
    "    .agg(spark_sum(\"Product.ItemCount\").alias(\"TotalCount\"))\n",
    "\n",
    "# Aynı ürünü birden çok kez alan müşteriler (TotalCount > 1)\n",
    "multiple_products_customers = customer_product_counts.filter(col(\"TotalCount\") > 1)\n",
    "\n",
    "# Birden fazla farklı ürün alan müşteriler (DÜZELTİLDİ: Product.ProductId yerine ProductId)\n",
    "multiple_products_customers_summary = multiple_products_customers.groupBy(\"UserId\") \\\n",
    "    .agg(countDistinct(\"ProductId\").alias(\"DistinctProducts\")) \\\n",
    "    .filter(col(\"DistinctProducts\") > 1)\n",
    "\n",
    "multiple_products_customers_summary.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dec4bb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "customer_product_counts.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f084ebe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "jdbc_url = \"jdbc:postgresql://postgres:5432/airflow\"\n",
    "connection_properties = {\n",
    "    \"user\": \"airflow\",\n",
    "    \"password\": \"airflow\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Örnek: product_sales DataFrame'ini Postgres'e yazma (önce truncate yapman gerekir)\n",
    "# Burada truncate için ayrı bir yol izlemen gerekebilir, Spark JDBC truncate desteklemez.\n",
    "\n",
    "# Yazma\n",
    "product_sales.write.jdbc(url=jdbc_url, table=\"product_sales\", mode=\"overwrite\", properties=connection_properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ff0afe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum as spark_sum, desc\n",
    "\n",
    "# SparkSession zaten varsa kullan, yoksa oluştur\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PostgresQuery\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/airflow\"\n",
    "connection_properties = {\n",
    "    \"user\": \"airflow\",\n",
    "    \"password\": \"airflow\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# product_sales tablosunu oku\n",
    "df = spark.read.jdbc(url=jdbc_url, table=\"product_sales\", properties=connection_properties)\n",
    "\n",
    "# En çok tekrar tekrar satın alınan en popüler ilk 10 ürün\n",
    "result = df.groupBy(\"ProductId\") \\\n",
    "    .agg(spark_sum(\"TotalSold\").alias(\"TotalPurchasedCount\")) \\\n",
    "    .orderBy(desc(\"TotalPurchasedCount\")) \\\n",
    "    .limit(10)\n",
    "\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b0ef81",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
