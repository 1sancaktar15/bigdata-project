![CI](https://github.com/1sancaktar15/bigdata-project/actions/workflows/ci.yml/badge.svg)

# Big Data Project

Bu proje, modern bÃ¼yÃ¼k veri teknolojileriyle gerÃ§ek zamanlÄ± veri Ã¼retimi, akÄ±ÅŸÄ±, iÅŸlenmesi, analizi ve sonuÃ§larÄ±n kalÄ±cÄ± veri tabanlarÄ±na kaydedilmesini kapsamaktadÄ±r. TÃ¼m bileÅŸenler Docker Ã¼zerinde entegre Ã§alÄ±ÅŸÄ±r ve aÅŸaÄŸÄ±daki iÅŸ akÄ±ÅŸÄ±nÄ± gerÃ§ekleÅŸtirir:

---

## Proje AkÄ±ÅŸÄ± ve BileÅŸenler

1. **FastAPI Web API**
    - KullanÄ±cÄ± etkinlikleri ve satÄ±n alma iÅŸlemleri iÃ§in iki endpoint:
        - `PUT /SendEvent`: Tekil kullanÄ±cÄ± etkinliÄŸi (UserId, SessionId, EventName, TimeStamp, Attributes, ProductId, Price, Discount)
        - `POST /PurchasedItems`: Birden fazla satÄ±n alma kaydÄ± (SessionId, TimeStamp, UserId, TotalPrice, OrderId, Products[ProductId, ItemCount, ItemPrice, ItemDiscount], PaymentType)
    - Gelen verileri Kafkaâ€™da iki ayrÄ± topicâ€™e iletir: `UserEvents` ve `PurchasedItem`.

2. **Data Generator**
    - Faker kÃ¼tÃ¼phanesi ile rastgele UserEvent ve PurchasedItem verisi Ã¼retir.
    - Her saniye FastAPIâ€™ye yeni kayÄ±tlar gÃ¶nderir.

3. **Apache Kafka**
    - GerÃ§ek zamanlÄ± veri akÄ±ÅŸÄ± iÃ§in iki topic: `UserEvents` ve `PurchasedItem`.

4. **Airflow DAG**
    - Her 2 dakikada bir Kafkaâ€™dan UserEvents verilerini toplar ve MongoDBâ€™ye kaydeder.
    - ArdÄ±ndan MongoDBâ€™de kullanÄ±cÄ± baÅŸÄ±na event tÃ¼rÃ¼ sayÄ±sÄ±nÄ± aggregate edip baÅŸka bir collectionâ€™a yazar.

5. **MongoDB**
    - UserEvents ve event count aggregation sonuÃ§larÄ±nÄ± saklar.

6. **PySpark Streaming**
    - Kafkaâ€™daki `PurchasedItem` topicâ€™ine subscribe olur.
    - Gelen satÄ±n alma verilerini MinIO S3 bucketâ€™Ä±na Parquet formatÄ±nda yazar.

7. **MinIO (S3 Uyumlu Depolama)**
    - PySpark ile yazÄ±lan satÄ±n alma verilerini saklar.

8. **Jupyter Notebook ile Analiz**
    - MinIOâ€™daki Parquet dosyalarÄ±nÄ± Spark ile okur.
    - Analizler:
        - En Ã§ok satÄ±lan Ã¼rÃ¼nler
        - En Ã§ok tercih edilen Ã¶deme tipi
        - Son 1 saatte en yÃ¼ksek tutarlÄ± sipariÅŸi veren top 10 mÃ¼ÅŸteri
        - AynÄ± Ã¼rÃ¼nÃ¼ birden Ã§ok kez satÄ±n alan mÃ¼ÅŸteriler ve Ã¼rÃ¼nler (**bu analiz Postgresâ€™e yazÄ±lÄ±r**)
    - Analiz sonuÃ§larÄ±nÄ± PostgreSQLâ€™e kaydeder.

9. **PostgreSQL**
    - Analiz sonuÃ§larÄ±nÄ±n kalÄ±cÄ± olarak saklandÄ±ÄŸÄ± iliÅŸkisel veritabanÄ±.
    - Ek bir notebook ile, Postgresâ€™teki verilerden:
        - En Ã§ok tekrar tekrar satÄ±n alÄ±nan en popÃ¼ler ilk 10 Ã¼rÃ¼n SQL ile bulunur.

---

## KullanÄ±lan Teknolojiler

- FastAPI
- Faker (Data Generator)
- Apache Kafka
- Apache Airflow
- MongoDB
- Apache Spark (PySpark)
- MinIO
- PostgreSQL
- Jupyter Notebook
- Docker & Docker Compose

---

## Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

1. **TÃ¼m servisleri baÅŸlatÄ±n:**
    ```
    docker-compose up -d
    ```
2. **Data Generatorâ€™Ä± baÅŸlatÄ±n:**  
   (APIâ€™ye sÃ¼rekli veri gÃ¶ndermeye baÅŸlar)
3. **Airflow, Spark ve diÄŸer servislerin loglarÄ±nÄ± kontrol edin.**
4. **Jupyter Notebookâ€™u aÃ§Ä±p analiz adÄ±mlarÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n.**
5. **PostgreSQLâ€™de analiz sonuÃ§larÄ±nÄ± ve SQL sorgularÄ±nÄ± inceleyin.**

---

## Dizin YapÄ±sÄ±

- `docker-compose.yaml` : TÃ¼m servislerin Docker konfigÃ¼rasyonu
- `fastapi-app/` : FastAPI uygulamasÄ± kodlarÄ±
- `data-generator/` : Faker ile veri Ã¼reten Python scripti
- `airflow/` : Airflow DAGâ€™larÄ± ve konfigÃ¼rasyonlarÄ±
- `spark-app/` : PySpark ile Kafkaâ€™dan MinIOâ€™ya veri yazan kod
- `notebooks/` : Analiz ve SQL notebooklarÄ±

---

## Analizler ve SonuÃ§lar

- **En Ã§ok satÄ±lan Ã¼rÃ¼nler**
- **En Ã§ok tercih edilen Ã¶deme tipi**
- **Son 1 saatte en yÃ¼ksek tutarlÄ± sipariÅŸi veren top 10 mÃ¼ÅŸteri**
- **AynÄ± Ã¼rÃ¼nÃ¼ birden Ã§ok kez satÄ±n alan mÃ¼ÅŸteriler ve Ã¼rÃ¼nler** (Postgresâ€™e yazÄ±lÄ±r)
- **En Ã§ok tekrar tekrar satÄ±n alÄ±nan ilk 10 Ã¼rÃ¼n** (SQL ile Postgresâ€™ten Ã§ekilir)

---


# Ekstra GeliÅŸtirmeler (Ã–dev KapsamÄ± DÄ±ÅŸÄ±nda)

Proje gereksinimlerinin Ã¶tesinde yapÄ±lan ek analiz ve gÃ¶rselleÅŸtirme Ã§alÄ±ÅŸmalarÄ±:

- PySpark ile MinIOâ€™daki Parquet dosyalarÄ±nÄ±n okunmasÄ±  
- ÃœrÃ¼n verilerinin `explode` edilerek pandas DataFrameâ€™e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmesi  
- En Ã§ok satÄ±lan 10 Ã¼rÃ¼nÃ¼n bar grafiÄŸi ile gÃ¶rselleÅŸtirilmesi  
- Ã–deme tiplerinin pasta grafiÄŸi ile sunulmasÄ±  
- Grafiklerde estetik ve okunabilirlik odaklÄ± iyileÅŸtirmeler yapÄ±ldÄ± ve kaydedildi.

### En Ã‡ok SatÄ±lan ÃœrÃ¼nler
![Top Products](notebooks/top_10_products.png)

### Ã–deme TÃ¼rleri
![Ã–deme TÃ¼rleri](notebooks/payment_types.png)

---

## ğŸ›°ï¸ Airflow ve PySpark Slack Bildirim Otomasyonu

Bu projede uÃ§tan uca veri akÄ±ÅŸÄ± ve otomasyonunu takip edebilmeniz iÃ§in, **Slack bildirimleri** hem **Airflow** hem de **PySpark** tarafÄ±ndan otomatik olarak gÃ¶nderilmektedir.


### ğŸ“¡ 1. Airflow DAGâ€™Ä± ile Bildirim

- **Kafka**â€™dan `UserEvents` verileri toplanÄ±r.
- Veriler **MongoDB**â€™ye kaydedilir ve **aggregation** iÅŸlemleri gerÃ§ekleÅŸtirilir.
- TÃ¼m iÅŸlemler baÅŸarÄ±yla tamamlandÄ±ÄŸÄ±nda, **Slack kanalÄ±na otomatik bildirim** gider.

#### ğŸ¯ Ã–rnek Bildirim MesajÄ±:
```text
âœ… Airflow: UserEvents aggregation baÅŸarÄ±yla tamamlandÄ±!
```


### âš¡ 2. PySpark ile BÃ¼yÃ¼k Tutar AlÄ±ÅŸveriÅŸ Bildirimi

- **PySpark**, Kafkaâ€™daki `PurchasedItem` topicâ€™inden alÄ±ÅŸveriÅŸ verilerini **sÃ¼rekli izler**.
- Toplam tutarÄ± **10.000 TL**â€™den bÃ¼yÃ¼k olan alÄ±ÅŸveriÅŸleri algÄ±lar ve Slackâ€™in `#alerts` kanalÄ±na otomatik olarak bildirir.
- KÃ¼Ã§Ã¼k iÅŸlemler iÃ§in bildirim gÃ¶nderilmez; yalnÄ±zca dikkat Ã§ekici iÅŸlemler Ã¶ne Ã§Ä±kar.

#### ğŸ¯ Ã–rnek PySpark Bildirim MesajÄ±:
```text
ğŸš¨ BÃ¼yÃ¼k AlÄ±ÅŸveriÅŸ UyarÄ±sÄ±!
KullanÄ±cÄ±: 2fef2298-d574-4cfd-b106-07a3d1a3da35
SipariÅŸ ID: caffa447-bee2-49c2-a573-088d68ad6f17
Tutar: 10618.65 TL
```


## ğŸ’¬ Slackâ€™te Gelen Otomatik Bildirimler

AÅŸaÄŸÄ±da, sistemin Ã§alÄ±ÅŸÄ±r durumda olduÄŸu ve yÃ¼ksek tutarlÄ± alÄ±ÅŸveriÅŸlerde Slackâ€™e baÅŸarÄ±lÄ± ÅŸekilde bildirim gÃ¶nderdiÄŸi bir kanal ekran gÃ¶rÃ¼ntÃ¼sÃ¼ bulunmaktadÄ±r:

![Slack Otomatik Bildirim](gorseller/SLACK.png)


Bu geliÅŸtirme ile, gerÃ§ek zamanlÄ± veri akÄ±ÅŸÄ± ve Ã¶nemli durumlar iÃ§in ekibe haber verme yeteneÄŸiyle **modern, profesyonel bir otomasyon Ã¶rneÄŸi** sunar. Hem analiz hem de monitoring iÃ§in kolayca geniÅŸletilebilir yapÄ±dadÄ±r.

---

## ğŸ›¡ Veri Kalitesi ve HatalÄ± Veri YÃ¶netimi

Veri akÄ±ÅŸÄ±nda dÃ¼ÅŸÃ¼k kaliteli, eksik ya da hatalÄ± verilerin Ã¼retim hattÄ±nÄ± bozmadan izlenmesi saÄŸlanmÄ±ÅŸtÄ±r.  
**PySpark Streaming** tarafÄ±nda, satÄ±n alma verileri alÄ±nÄ±rken gelen kayÄ±tlar anlÄ±k olarak kontrol edilir:

- **GeÃ§erli (Valid) kayÄ±tlar:**  
  Zorunlu alanlarÄ± (SessionId, UserId, OrderId, TotalPrice) eksiksiz ve TotalPrice > 0 olanlar,  
  otomatik olarak MinIOâ€™daki `purchased-items/valid/` klasÃ¶rÃ¼ne Parquet formatÄ±nda yazÄ±lÄ±r.

- **HatalÄ± (Invalid) kayÄ±tlar:**  
  Eksik, boÅŸ, null veya TotalPrice â‰¤ 0 olan kayÄ±tlar,  
  veri kaybÄ± olmamasÄ± amacÄ±yla `purchased-items/invalid/` klasÃ¶rÃ¼ne yazÄ±lÄ±r.

Bu sayede:
- Ä°ÅŸ akÄ±ÅŸÄ± sadece kaliteli verilerle devam eder.
- HatalÄ± kayÄ±tlar izlenebilir ve gerektiÄŸinde analiz iÃ§in kullanÄ±labilir.


---

## KatkÄ± ve Ä°letiÅŸim

Her tÃ¼rlÃ¼ soru ve Ã¶neriniz iÃ§in [GitHub Issues](https://github.com/1sancaktar15/bigdata-project/issues) Ã¼zerinden iletiÅŸime geÃ§ebilirsiniz.

---

> Proje dokÃ¼mantasyonu ve iÅŸ akÄ±ÅŸÄ±, [ProjeOdeviBigData.pdf][1] dokÃ¼manÄ±ndaki gereksinimleri tam olarak karÅŸÄ±layacak ÅŸekilde hazÄ±rlanmÄ±ÅŸtÄ±r.

[1]: https://github.com/1sancaktar15/bigdata-project/blob/main/ProjeOdeviBigData.pdf
